{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este c칩digo deber치 correrse en una m치quina remota que pueda soportarlo. Funcion칩 en una Ada 5000\n",
    "\n",
    "No olvidar cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from evo2 import Evo2\n",
    "import pandas as pd\n",
    "\n",
    "evo2_model = Evo2('evo2_7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1 = pd.read_excel(\"41586_2018_461_MOESM3_ESM.xlsx\", header=2)\n",
    "brca1 = brca1[[\n",
    "    'chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class',\n",
    "]]\n",
    "\n",
    "brca1.rename(columns={\n",
    "    'chromosome': 'chrom',\n",
    "    'position (hg19)': 'pos',\n",
    "    'reference': 'ref',\n",
    "    'alt': 'alt',\n",
    "    'function.score.mean': 'score',\n",
    "    'func.class': 'class',\n",
    "}, inplace=True)\n",
    " \n",
    "brca1['class'] = brca1['class'].replace(['FUNC', 'INT'], 'FUNC/INT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing\n",
    "sequence = 'ACGTACGTACGTACGTACGT'\n",
    "input_ids = torch.tensor(\n",
    "    evo2_model.tokenizer.tokenize(sequence),\n",
    "    dtype=torch.int,\n",
    ").unsqueeze(0).to('cuda:0')\n",
    "\n",
    "layer_name = 'blocks.28.mlp.l3'\n",
    "\n",
    "outputs2, embeddings2 = evo2_model(input_ids, return_embeddings=True, layer_names=[layer_name])\n",
    "\n",
    "print('Embeddings shape: ', embeddings2[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 128\n",
    "complement_table = str.maketrans(\"ATCG\", \"TAGC\")\n",
    "def parse_sequences2(pos, ref, alt, chrn = \"chr17\"):\n",
    "    match chrn:\n",
    "        case \"chr17\":\n",
    "            p = pos - 1 \n",
    "            full_seq = chr17\n",
    "        case \"chr13\":\n",
    "            p = pos\n",
    "            full_seq = chr13\n",
    "\n",
    "    ref_seq_start = p - WINDOW_SIZE//2\n",
    "    ref_seq_end = p + WINDOW_SIZE//2\n",
    "    ref_seq = full_seq[ref_seq_start:ref_seq_end]\n",
    "    snv_pos_in_ref = min(WINDOW_SIZE//2, p)\n",
    "    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref+1:]\n",
    "\n",
    "    ref_seq_rev_comp = ref_seq.translate(complement_table)[::-1]\n",
    "    var_seq_rev_comp = var_seq.translate(complement_table)[::-1]\n",
    "\n",
    "    assert len(var_seq) == len(ref_seq)\n",
    "    assert ref_seq[snv_pos_in_ref] == ref\n",
    "    assert var_seq[snv_pos_in_ref] == alt\n",
    "\n",
    "    return ref_seq, var_seq, ref_seq_rev_comp, var_seq_rev_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "layer_name = 'blocks.28.mlp.l3'\n",
    "idx_embd = {}\n",
    "\n",
    "for idx, row in tqdm(brca1.iterrows(), total=len(brca1), desc=\"Procesando variantes de BRCA1\"):\n",
    "    ref_seq, var_seq, ref_seq_rev_comp, var_seq_rev_comp = parse_sequences2(row['pos'], row['ref'], row['alt'], \"chr17\")\n",
    "    sequences_list = [ref_seq, var_seq, ref_seq_rev_comp, var_seq_rev_comp]\n",
    "    input_ids = torch.tensor(\n",
    "        [evo2_model.tokenizer.tokenize(s) for s in sequences_list],\n",
    "        dtype=torch.int,\n",
    "    ).to('cuda:0')\n",
    "    _ , embeddings = evo2_model(input_ids, return_embeddings=True, layer_names=[layer_name])\n",
    "\n",
    "    idx_embd[idx] = embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_avg = {idx: tensor['blocks.28.mlp.l3'].mean(dim=1) for idx, tensor in idx_embd.items()}\n",
    "embeddings_f = {idx: embd.flatten() for idx, embd in embeddings_avg.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'brca1_embeddings.pth'\n",
    "torch.save(embeddings_f, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
